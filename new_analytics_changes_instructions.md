Copilot Implementation Instructions
The implementation will follow a data-first approach: extend the models, create the data capture, implement the calculation service, and finally, build the front-end display.

Phase 1: Backend (Node.js/Mongoose) — Data Modeling & Capture
Instruct Copilot to update the core models to store the granular data needed for your custom metrics.

1. Extend server/src/models/Attempt.js

Field	Type	Default/Constraints	Purpose (For Metrics)	Spec Link
durationSeconds	Number	required: true	Time Taken per Attempt	Implied by
isTimed	Boolean	default: true	Used for Time Constraint analysis.	Implied by
deviceType	String	enum: ['desktop', 'tablet', 'mobile']	Used for Device Mix admin metric.	
analysisNotes	String	maxlength: 2000, default: ''	Store the qualitative data from the Results Page.	
q_details	Array (Embedded Schema)		Stores granular question data.	Derived

Export to Sheets
2. Create Embedded q_details Schema (inside Attempt.js)

Field	Type	Purpose (For Metrics)
questionIndex	Number	Reference.
timeSpent	Number	Time Taken per Question (for Pacing analysis).
wasReviewed	Boolean	Tracked via MFR system for Reviews analysis.
isCorrect	Boolean	For Overall Accuracy and rollups.
qType	String	e.g., 'Inference', 'Direct' (for Accuracy by Question Type).
qCategory	String	e.g., 'Fiction', 'Concept' (for Accuracy by Passage Type).

Export to Sheets
3. Implement Admin Data Logging

Instruction: Create middleware or a log service function that is called at login and attempt submission to capture the data needed for the admin metrics:

Login Time: Log userId and timestamp on successful login to calculate Login vs Attempt Gap.

Attempt Submission: Log userId, isTimed, and deviceType.

Phase 2: Frontend (React) — Data Capture
Instruct Copilot to implement the client-side data tracking using local component state before sending to the server.

1. <TestPage /> (client/src/features/test/Test.jsx)

Time Tracking: Implement a useEffect hook to start a timer on component mount. Update local state with timeSpentOnPassage when the user clicks the first answer. Use a per-question timer that stops/starts when the questionIndex changes.

Final Submission: On "Submit Test", bundle the following for the new Attempt API payload:

durationSeconds (Total time)

deviceType (From navigator.userAgent)

q_details (The array of per-question metrics collected above)

2. <ReasonTagSelect /> (client/src/features/results/ReasonTagSelect.jsx)

Instruction: Ensure the onReasonSelected handler calls the new PATCH /api/attempts/:id/reasons endpoint.

UX: Implement the optimistic update showing a subtle checkmark upon successful save.

Phase 3: Backend (Node.js) — Analytics Service
The new server/src/services/analytics.service.js will compute all advanced metrics.

1. Update buildQuestionRollups(userId, range)

Instruction: Modify this function to use the new q_details array from the Attempt model.

Grouping Logic: Group raw questions by both qType (Question Type) and qCategory (Passage Type) to calculate Accuracy by Question/Passage Type.

2. Create computeAdvancedMetrics(attempts)

Instruction: Create a new function that takes a collection of recent attempts and computes your custom metrics:

Top Reason Codes/Untagged: Use the wrongReasons array and totalWrong count to directly compute Top Reason Codes and Untagged Wrong Answers (as the complement of Reason Coverage).

Accuracy Momentum: Use the accuracy data from buildProgressTimeline to calculate the 30-day slope (a simple average percentage change week-over-week).

3. Update GET /api/dashboard

Instruction: Ensure the unified dashboard endpoint calls the new computeAdvancedMetrics function to embed the required data points:

stats.coverage (for Untagged Wrong Answers)

reasons.top (for Top Reason Codes)

Phase 4: Frontend (React) — Visualization
1. Dashboard Enhancements

Instruction: Update <DashboardStatsRow /> to display the Attempts per IST Day (as attempts7d) and the Untagged Wrong Answers (as coverage percentage).

Instruction: Use the reasons.top data to populate the <ReasonSummaryWidget />.

2. Performance Studio (/performance)

Instruction: Build the tables and charts (Radar, Question Type Table) using the data generated by the enhanced buildQuestionRollups function to display Accuracy by Question Type and Accuracy by Passage Type.



new analytics list 

. Student-Facing Metrics: Analysis & Derivation
Metric	Derivation & Support in Specification	New Feature Link
Overall Accuracy	Directly supported by buildProgressTimeline and assemblePerformancePayload. Displayed in the Performance Studio Callout Metric and on the Results Page Score Card.	Performance Studio, Results Score Card
Time Taken per Attempt	Stored implicitly in attempt data, displayed on the Results Page as "Duration".	Results Score Card Hero
Attempts per IST Day	Calculated by grouping the buildProgressTimeline data buckets by date. Displayed as bars on the Timeline Chart and as attempts7d on the Dashboard.	Analytics Service, Timeline Chart
Accuracy by Question Type	Directly supported by the buildQuestionRollups function, which aggregates data by questionType/category. Powers the Radar Chart and Question Type Table.	Radar Chart, Analytics Service
Accuracy by Passage Type	Similar to Question Type. Requires passageType/topic to be part of the buildQuestionRollups aggregator, which is supported by the new rollups architecture.	Analytics Service, Question Type Table
Explanation Consumption	Requires tracking user behavior (new tracking event). This is a behavioral metric that requires tracking clicks to "Expand to see explanation" on the Results Page.	Results Page Logic
Top Reason Codes	Directly supported by the buildReasonsSummary endpoint and powers the Dashboard Reason Summary Widget.	Weekly Reason Summary API, Dashboard Widget
Untagged Wrong Answers	Calculated as totalWrong - taggedWrong. This is the inverse of the Reason Coverage metric.	Coverage Metric, Weekly Reason Summary API
Daily Streak & Consistency	Supported by the DashboardGreeting component, which displays streakDays. Consistency is also visualized by the Attempts bars in the Timeline Chart.	Dashboard Greeting, Progress Preview
Session Time of Day	Requires tracking time of day for attempts. The buildAttemptHistory function records date, allowing filtering by time of day (Morning/Afternoon/Evening) for analysis.	Attempt History, Dashboard Greeting (time-of-day logic)
Accuracy Momentum	The visual trend calculated by buildProgressTimeline and displayed on the Timeline Chart and Sparkline. Mathematically, this could be the slope of the accuracy line over the last 30 days.	Progress Timeline, Progress Preview
Time-to-Mastery per passage Type	Requires calculating the point at which a student's accuracy (by passage type) consistently exceeds a target (e.g., 80%). This is a complex derivative of buildQuestionRollups data over time.	Analytics Service

Export to Sheets
II. Admin-Facing Metrics: Platform Health & Content
Metric	Derivation & Support in Specification	New Feature Link
Completion Rate	Requires tracking started attempts vs. submitted attempts. Data exists within the Attempt model (or implied attempt data) to measure this funnel.	Attempt Model Logic
Device Mix	Requires adding deviceType to attempt tracking. Not explicitly in the spec, but a necessary log for the layout optimization goal.	Test Layout Optimization
Login vs Attempt Gap	Requires combining User Login timestamps (Auth) and Attempt Creation timestamps. This measures the delay between engagement and practice.	Auth Rate Limiting (login time data implied), Attempt History
Practice-to-Official Conversion	Requires tracking if a practice set was later converted into a full 'official' test. This is an advanced feature relying on the core test experience data flow.	Attempt History, Test Experience